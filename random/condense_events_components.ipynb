{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import zipfile\n",
    "import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "root_path = os.path.join(\"event\")\n",
    "result_path = os.path.join(\"event_drug_symptom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3074136\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(\"drug\",\"no_missing_drugs_v2.csv\"))\n",
    "subset = df[df[\"postings\"].str.contains(\" \")] # find events with multiple drugs\n",
    "print(len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_year = {\n",
    "            '800': 10,          # decade\n",
    "            '801': 1,           # year\n",
    "            '802': 1 / 12,      # month\n",
    "            '803': 1 / (12 * 4),# week\n",
    "            '804': 1 / 365,     # days\n",
    "            '805': 1 / 8760,    # hour\n",
    "        }\n",
    "decode_sex = {0: 'unknown', 1: \"Male\", 2: \"Female\"}\n",
    "\n",
    "decode_characterization = {\n",
    "            1: \"Suspect (the drug was considered by the reporter to be the cause)\",\n",
    "            2: \"Concomitant (the drug was reported as being taken along with the suspect drug)\",\n",
    "            3: \"Interacting (the drug was considered by the reporter to have interacted with the suspect drug)\"\n",
    "        }\n",
    "decode_serious = {\n",
    "            1: \"The adverse event resulted in death, a life threatening condition, hospitalization, disability, \\\n",
    "            congenital anomaly, or other serious condition\",\n",
    "            2: \"The adverse event did not result in any of the above\"}\n",
    "decode_death = {\n",
    "            0: \"survive\",\n",
    "            1: \"death\"\n",
    "        }\n",
    "decode_sender = {\n",
    "            1: \"Physician\",\n",
    "            2: \"Pharmacist\",\n",
    "            3: \"Other health professional\",\n",
    "            4: \"Lawyer\",\n",
    "            5: \"Consumer or non-health professional\",\n",
    "            0: \"unknown\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from download.py with modification\n",
    "def download(arg):\n",
    "    \"\"\"download file at url and save at dir \"\"\"\n",
    "    url, directory = arg\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(directory, 'wb').write(r.content)\n",
    "    with zipfile.ZipFile(directory, 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.dirname(directory))\n",
    "    os.remove(directory)\n",
    "\n",
    "def get_event_links(path):\n",
    "    \"\"\"obtain all download links for events from fdc's provided download json\"\"\"\n",
    "    with open(path, 'r') as handle:\n",
    "        file = json.load(handle)\n",
    "    events = file['results'][\"drug\"][\"event\"]\n",
    "    links = []\n",
    "    for p in events['partitions']:\n",
    "        links.append(p['file'])\n",
    "    return links\n",
    "\n",
    "def get_ndc_links(path):\n",
    "    \"\"\"obtain all download links for events from fdc's provided download json\"\"\"\n",
    "    with open(path, 'r') as handle:\n",
    "        file = json.load(handle)\n",
    "    events = file['results'][\"drug\"][\"ndc\"]\n",
    "    links = []\n",
    "    for p in events['partitions']:\n",
    "        links.append(p['file'])\n",
    "    return links\n",
    "\n",
    "links = get_event_links(\"download.json\")\n",
    "ndc_links = get_ndc_links(\"download.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ndc file\n",
    "url = ndc_links[0]\n",
    "infos = url.split(\"/\")[3:] # getting rid of https and website\n",
    "path = ''\n",
    "for i in infos:\n",
    "    path = os.path.join(path, i)\n",
    "    if i.endswith('.zip'):\n",
    "        break\n",
    "    else:\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        path = path.strip(\"\\n\")\n",
    "        url = url.strip(\"\\n\")\n",
    "download([url, path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21099it [00:00, 2350481.28it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create a symptom database from symptoms.json\"\"\"\n",
    "\n",
    "symptoms = json.load(open(\"symptoms.json\", \"r\"))\n",
    "data = {\"id\":[], \"symptom\":[]}\n",
    "for i, ele in tqdm(enumerate(symptoms)):\n",
    "    data[\"id\"].append(i)\n",
    "    data[\"symptom\"].append(ele)\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(os.path.join(\"drug\",\"symptoms_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126675it [00:00, 460194.93it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create datasets for ndc's drug listings and their active ingredients\"\"\" \n",
    "\"\"\"id is associated with the brand_name\"\"\"\n",
    "\n",
    "ndc_data = {\"id\":[], \"brand_name\":[], \"generic_name\":[], \"product_type\":[], \"manufacturer_name\":[]}\n",
    "comp_data = {\"id\":[], \"active_ingredient\":[]}\n",
    "\n",
    "encode_product_type = {\"HUMAN OTC DRUG\":0, \n",
    "                       \"HUMAN PRESCRIPTION DRUG\":1, \n",
    "                       \"PLASMA DERIVATIVE\":2, \n",
    "                       \"VACCINE\":3, \n",
    "                       \"BULK INGREDIENT\":4, \n",
    "                       \"DRUG FOR FURTHER PROCESSING\":5, \n",
    "                       \"CELLULAR THERAPY\":6, \n",
    "                       \"LICENSED VACCINE BULK INTERMEDIATE\":7,\n",
    "                       \"STANDARDIZED ALLERGENIC\":8, \"NON-STANDARDIZED ALLERGENIC\":9}\n",
    "\n",
    "with open(os.path.join(\"drug\",\"ndc\", \"drug-ndc-0001-of-0001.json\")) as ifs:\n",
    "    ndc = json.load(ifs)\n",
    "    ndc = ndc[\"results\"] # list of dict\n",
    "    for i, ele in tqdm(enumerate(ndc)):\n",
    "        if \"generic_name\" in ele:\n",
    "            for name in ele[\"generic_name\"].split(\",\"):\n",
    "                ndc_data[\"id\"].append(i)\n",
    "                ndc_data[\"generic_name\"].append(name.strip())\n",
    "                ndc_data[\"brand_name\"].append(ele[\"brand_name_base\"])\n",
    "                ndc_data[\"product_type\"].append(encode_product_type[ele[\"product_type\"]])\n",
    "                if \"manufacturer_name\" in ele[\"openfda\"]:\n",
    "                    ndc_data[\"manufacturer_name\"].append(ele[\"openfda\"][\"manufacturer_name\"][0])\n",
    "                else:\n",
    "                    ndc_data[\"manufacturer_name\"].append(\"__NULL__\")\n",
    "        else:\n",
    "            ndc_data[\"id\"].append(i)\n",
    "            ndc_data[\"generic_name\"].append(\"__NULL__\")\n",
    "            ndc_data[\"brand_name\"].append(ele[\"brand_name_base\"])\n",
    "            ndc_data[\"product_type\"].append(encode_product_type[ele[\"product_type\"]])\n",
    "            if \"manufacturer_name\" in ele[\"openfda\"]:\n",
    "                ndc_data[\"manufacturer_name\"].append(ele[\"openfda\"][\"manufacturer_name\"][0])\n",
    "            else:\n",
    "                ndc_data[\"manufacturer_name\"].append(\"__NULL__\")\n",
    "        if \"active_ingredients\" in ele:\n",
    "            for comp in ele[\"active_ingredients\"]:\n",
    "                comp_data[\"id\"].append(i)\n",
    "                comp_data[\"active_ingredient\"].append(comp[\"name\"])\n",
    "    ndc_df = pd.DataFrame(ndc_data)\n",
    "    ndc_df.to_csv(os.path.join(\"drug\",\"drug_dataset.csv\"), index=False)\n",
    "    comp_df = pd.DataFrame(comp_data)\n",
    "    comp_df.to_csv(os.path.join(\"drug\",\"active_ingredient_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223302it [02:28, 1505.09it/s]\n",
      "205302it [00:15, 13315.06it/s]\n",
      "21099it [00:01, 16386.84it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Drug and Symptom Reverse Lookup\"\"\"\n",
    "\n",
    "class Drug:\n",
    "    def __init__(self, components=None):\n",
    "        self.components = set(components) # components are lowercase strings\n",
    "    def __str__(self):\n",
    "        return ' '.join([str(x) for x in self.components])\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "    def __len__(self):\n",
    "        return len(self.components)\n",
    "    def __list__(self):\n",
    "        return list(self.components)\n",
    "    def __eq__(self, other):\n",
    "        return self.components == other.components\n",
    "\n",
    "components = {}\n",
    "drug_to_comp = {}\n",
    "comp_id = 0\n",
    "comp_df = pd.read_csv(os.path.join(\"drug\",\"active_ingredient_dataset.csv\"))\n",
    "for i,row in tqdm(comp_df.iterrows()):\n",
    "    subset = comp_df.loc[comp_df[\"id\"] == row[\"id\"]]\n",
    "    comps = subset[\"active_ingredient\"].values\n",
    "    comps = Drug([x.lower() for x in comps])\n",
    "    if comps not in components:\n",
    "        components[comps] = comp_id\n",
    "        comp_id += 1\n",
    "    drug_to_comp[row[\"id\"]] = components[comps]\n",
    "num_drugs = len(components)\n",
    "\n",
    "#Brand_Name -> component set\n",
    "drug_vocab = {}\n",
    "drug_df = pd.read_csv(os.path.join(\"drug\",\"drug_dataset.csv\"))\n",
    "for i,row in tqdm(drug_df.iterrows()):\n",
    "    if row[\"id\"] in drug_to_comp:\n",
    "        try:\n",
    "            drug_vocab[row[\"brand_name\"].lower()] = drug_to_comp[row[\"id\"]]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            drug_vocab[row[\"generic_name\"].lower()] = drug_to_comp[row[\"id\"]]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Keep track of discovered drugs\n",
    "\n",
    "missing_vocab = {}\n",
    "missing_drugs = {\"id\":[], \"brand_name\":[]}\n",
    "\n",
    "# Symptom -> ID\n",
    "symptom_vocab = {}\n",
    "symptom_df = pd.read_csv(os.path.join(\"drug\",\"symptoms_dataset.csv\"))\n",
    "symptom_df['symptom'] = symptom_df['symptom'].str.lower()\n",
    "for i,row in tqdm(symptom_df.iterrows()):\n",
    "    symptom_vocab[row[\"symptom\"]] = row[\"id\"]\n",
    "    \n",
    "event_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(\"drug\",\"events\")):\n",
    "    os.mkdir(os.path.join(\"drug\",\"events\"))\n",
    "    os.mkdir(os.path.join(\"drug\",\"drugs\"))\n",
    "    os.mkdir(os.path.join(\"drug\",\"characteristics\"))\n",
    "    os.mkdir(os.path.join(\"drug\",\"symptoms\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [12:44<00:00, 15.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Drug Dataset:\n",
    "    # ID (event)\n",
    "    # postings list based off drug_dataset\n",
    "    # missing list based off missing_dataset\n",
    "# Characterization dataset:\n",
    "    # ID (event)\n",
    "    # postings list based off drug_dataset\n",
    "    # missing list based off missing_dataset\n",
    "# Symptoms dataset:\n",
    "    # ID (event)\n",
    "    # postings list based off symptoms_dataset\n",
    "    \n",
    "def func(v):\n",
    "    # Check if drug in ndc data\n",
    "    if v in drug_vocab:\n",
    "        d.append(drug_vocab[v])\n",
    "        if \"drugcharacterization\" in ele: \n",
    "            c.append(ele[\"drugcharacterization\"])\n",
    "        else:\n",
    "            c.append(0)\n",
    "        return True\n",
    "    # Check if drug is a known component\n",
    "    elif Drug([v]) in components:\n",
    "        d.append(components[Drug([v])])\n",
    "        if \"drugcharacterization\" in ele: \n",
    "            c.append(ele[\"drugcharacterization\"])\n",
    "        else:\n",
    "            c.append(0)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#for link_index, url in tqdm(enumerate(links)):\n",
    "start = 800\n",
    "end = min(900, len(links))\n",
    "for link_index in tqdm(range(start, end)):\n",
    "    url = links[link_index]\n",
    "    # Download an event file\n",
    "    infos = url.split(\"/\")[3:] # getting rid of https and website\n",
    "    path = ''\n",
    "    for i in infos:\n",
    "        path = os.path.join(path, i)\n",
    "        if i.endswith('.zip'):\n",
    "            break\n",
    "        else:\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            path = path.strip(\"\\n\")\n",
    "            url = url.strip(\"\\n\")\n",
    "    download([url, path])\n",
    "\n",
    "    folder_name = path.split(\"\\\\\")[-2]\n",
    "    file_name = path.split(\".\")[0]\n",
    "    split_name = file_name.split(\"\\\\\")[-1]\n",
    "\n",
    "    # Grab and decode events\n",
    "    events = json.load(open(file_name+\".json\", \"r\"))\n",
    "    events = events[\"results\"]\n",
    "\n",
    "    # Event Dataset:\n",
    "    event_data = {\"ID\":[], \"sex\":[], \"age\":[], \"serious\":[], \"death\":[], \"sender\":[], \"time\":[]}\n",
    "    drug_data = {\"ID\":[], \"postings\":[], \"missing_imp\":[], \"missing_not\":[]}\n",
    "    char_data = {\"ID\":[], \"postings\":[], \"missing_imp\":[], \"missing_not\":[]}\n",
    "    symp_data = {\"ID\":[], \"postings\":[]}\n",
    "\n",
    "    for event in events:\n",
    "        patient = event[\"patient\"]\n",
    "\n",
    "        # Event\n",
    "        event_data[\"ID\"].append(event_id)\n",
    "        event_data[\"serious\"].append(event[\"serious\"])\n",
    "\n",
    "        if \"patientsex\" in patient:\n",
    "            event_data[\"sex\"].append(decode_sex[int(patient['patientsex'])])\n",
    "        else:\n",
    "            event_data[\"sex\"].append(0)\n",
    "        try:\n",
    "            event_data[\"age\"].append(int(patient[\"patientonsetage\"]) * to_year[patient[\"patientonsetageunit\"]])\n",
    "        except:\n",
    "            event_data[\"age\"].append(-1)\n",
    "        if \"seriousnessdeath\" in event:\n",
    "            event_data[\"death\"].append(1)\n",
    "        else:\n",
    "            event_data[\"death\"].append(0)\n",
    "        try:\n",
    "            event_data[\"sender\"].append(decode_sender[int(event[\"primarysource\"][\"qualification\"])])\n",
    "        except:\n",
    "            event_data[\"sender\"].append(0)\n",
    "        try:\n",
    "            time = event['receiptdate']\n",
    "            timestamp = datetime.datetime(int(time[0:4]), int(time[4:6]), int(time[6::]), 0, 0).timestamp()\n",
    "            event_data[\"time\"].append(timestamp)\n",
    "        except:\n",
    "            event_data[\"time\"].append(-1)\n",
    "\n",
    "        # Drugs & Characteristics\n",
    "        drug_data[\"ID\"].append(event_id)\n",
    "        char_data[\"ID\"].append(event_id)\n",
    "        d = []\n",
    "        c = []\n",
    "        \n",
    "        # Track missing drugs\n",
    "        # 1. Suspect\n",
    "        # 2. Concomitant\n",
    "        # 3. Interacting\n",
    "        # 0. Not given\n",
    "        \n",
    "        # Keep track of suspect / interacting drugs\n",
    "        m_d_imp = []\n",
    "        m_c_imp = []\n",
    "        \n",
    "        # Versus concomitant / unknown drugs\n",
    "        m_d_not = []\n",
    "        m_c_not = []\n",
    "        for ele in patient[\"drug\"]:\n",
    "            if \"medicinalproduct\" in ele:\n",
    "                if type(ele[\"medicinalproduct\"]) != str:\n",
    "                    continue\n",
    "                    \n",
    "                _drug = ele[\"medicinalproduct\"].lower()\n",
    "                \n",
    "                if len(re.findall(\"(unknown)|(unspecified)|(vitamin)|(inhaler)|(chewable)|(tea)\", _drug)) > 0:\n",
    "                    continue\n",
    "                \n",
    "                if not func(_drug):\n",
    "                    # Pattern 1: Split on (\n",
    "                    val = _drug.split(\"(\")[0]\n",
    "                    if func(val):\n",
    "                        continue\n",
    "                    \n",
    "                    # Patern 1.5: Split on -\n",
    "                    val = _drug.split(\"-\")[0]\n",
    "                    if func(val):\n",
    "                        continue\n",
    "                    \n",
    "                    # Pattern 2: Grab within ()\n",
    "                    val = re.findall(\"\\((.+)\\)\", _drug)\n",
    "                    if len(val) > 0:\n",
    "                        if func(val[0]):\n",
    "                            continue\n",
    "                    \n",
    "                    # Pattern 3: Drug + Drug\n",
    "                    val = _drug.split(\" + \")\n",
    "                    if len(val) == 2:\n",
    "                        if Drug(val) in components:\n",
    "                            d.append(components[Drug(val)])\n",
    "                            if \"drugcharacterization\" in ele: \n",
    "                                c.append(ele[\"drugcharacterization\"])\n",
    "                            else:\n",
    "                                c.append(0)\n",
    "                            continue\n",
    "                    \n",
    "                    # Pattern 4: Remove \"tablets\", \\d\\d\\d ?[um]g\n",
    "                    val = re.split(\" \\d\", _drug)[0]\n",
    "                    if func(val):\n",
    "                        continue\n",
    "                    \n",
    "                    val = re.split(\"( (?:tab)|(?:cap))\", _drug)\n",
    "                    if len(val) > 1:\n",
    "                        func(val[0])\n",
    "                        continue\n",
    "                        \n",
    "                        \n",
    "                    # ----- Drug Not Found -----\n",
    "                    \n",
    "                    # Check if this missing drug has been seen before\n",
    "                    if _drug not in missing_vocab:\n",
    "                        missing_drugs[\"id\"].append(num_drugs)\n",
    "                        missing_drugs[\"brand_name\"].append(_drug)\n",
    "                        missing_vocab[_drug] = num_drugs\n",
    "                        num_drugs += 1\n",
    "                    \n",
    "                    if \"drugcharacterization\" in ele:\n",
    "                        if ele[\"drugcharacterization\"] == \"1\" or ele[\"drugcharacterization\"] == \"3\":\n",
    "                            m_d_imp.append(missing_vocab[_drug])\n",
    "                            m_c_imp.append(ele[\"drugcharacterization\"])\n",
    "                        else:\n",
    "                            m_d_not.append(missing_vocab[_drug])\n",
    "                            m_c_not.append(ele[\"drugcharacterization\"])\n",
    "                    else:\n",
    "                        m_d_not.append(missing_vocab[_drug])\n",
    "                        m_c_not.append(0)                    \n",
    "                \n",
    "        drug_data[\"postings\"].append(\" \".join([str(x) for x in d]))\n",
    "        char_data[\"postings\"].append(\" \".join([str(x) for x in c]))\n",
    "        drug_data[\"missing_imp\"].append(\" \".join([str(x) for x in m_d_imp]))\n",
    "        char_data[\"missing_imp\"].append(\" \".join([str(x) for x in m_c_imp]))\n",
    "        drug_data[\"missing_not\"].append(\" \".join([str(x) for x in m_d_not]))\n",
    "        char_data[\"missing_not\"].append(\" \".join([str(x) for x in m_c_not]))\n",
    "\n",
    "        # Symptoms\n",
    "        symp_data[\"ID\"].append(event_id)\n",
    "        if \"reaction\" in patient:\n",
    "            s = []\n",
    "            for sym in patient[\"reaction\"]:\n",
    "                if \"reactionmeddrapt\" in sym:\n",
    "                    s.append(str(symptom_vocab[sym['reactionmeddrapt'].lower()]))\n",
    "            symp_data[\"postings\"].append(\" \".join(s))\n",
    "        else:\n",
    "            symp_data[\"postings\"].append(\"\")\n",
    "\n",
    "        event_id += 1\n",
    "\n",
    "    # Remove download\n",
    "    os.remove(file_name+\".json\")\n",
    "    os.rmdir(os.path.join(\"\\\\\".join(path.split(\"\\\\\")[0:3])))\n",
    "\n",
    "    # Write results to file\n",
    "    if not os.path.exists(os.path.join(\"drug\",\"events\", folder_name)):\n",
    "        os.mkdir(os.path.join(\"drug\",\"events\", folder_name))\n",
    "        os.mkdir(os.path.join(\"drug\",\"drugs\", folder_name))\n",
    "        os.mkdir(os.path.join(\"drug\",\"characteristics\", folder_name))\n",
    "        os.mkdir(os.path.join(\"drug\",\"symptoms\", folder_name))\n",
    "\n",
    "    event_df = pd.DataFrame(event_data)\n",
    "    event_df.to_csv(os.path.join(\"drug\", \"events\", folder_name, split_name+\".csv\"), index=False)\n",
    "\n",
    "    drug_df = pd.DataFrame(drug_data)\n",
    "    drug_df.to_csv(os.path.join(\"drug\", \"drugs\", folder_name, split_name+\".csv\"), index=False)\n",
    "\n",
    "    char_df = pd.DataFrame(char_data)\n",
    "    char_df.to_csv(os.path.join(\"drug\", \"characteristics\", folder_name, split_name+\".csv\"), index=False)\n",
    "\n",
    "    symp_df = pd.DataFrame(symp_data)\n",
    "    symp_df.to_csv(os.path.join(\"drug\", \"symptoms\", folder_name, split_name+\".csv\"), index=False)\n",
    "\n",
    "missing_df = pd.DataFrame(missing_drugs)\n",
    "missing_df.to_csv(os.path.join(\"drug\",\"missing_drugs_dataset_\"+str(start)+\"_\"+str(end)+\".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868\n"
     ]
    }
   ],
   "source": [
    "#missing_df = pd.DataFrame(missing_drugs)\n",
    "#missing_df = missing_df.drop_duplicates(subset=\"brand_name\")\n",
    "#print(missing_df.head(20))\n",
    "print(len(missing_df))\n",
    "#missing_df.to_csv(os.path.join(\"drug\",\"missing_drugs_dataset_697.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num missing: 454931\n",
      "num events: 10432358\n"
     ]
    }
   ],
   "source": [
    "print(\"num missing:\", len(missing_drugs[\"brand_name\"]))\n",
    "print(\"num events:\", event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████████████████████████████████████▋ | 61/62 [01:57<00:07,  7.19s/it]C:\\Users\\sjray\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 62/62 [02:03<00:00,  7.05s/it]\n"
     ]
    }
   ],
   "source": [
    "df = None\n",
    "check = False\n",
    "for folder in tqdm(os.listdir(os.path.join(\"drug\", \"drugs\"))):\n",
    "    for file in os.listdir(os.path.join(\"drug\", \"drugs\", folder)):\n",
    "        if check:\n",
    "            df = pd.concat([df, pd.read_csv(os.path.join(\"drug\", \"drugs\", folder, file))])\n",
    "        else:\n",
    "            df = pd.read_csv(os.path.join(\"drug\", \"drugs\", folder, file))\n",
    "            check = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7042330\n"
     ]
    }
   ],
   "source": [
    "df = df.fillna(\"\")\n",
    "subset = df.loc[df[\"missing_imp\"]==\"\"]\n",
    "subset = subset.loc[subset[\"postings\"]!=\"\"]\n",
    "subset = subset[[\"ID\",\"postings\"]]\n",
    "print(len(subset))\n",
    "subset.to_csv(os.path.join(\"drug\",\"no_missing_drugs_v2.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data ={\"id\":[], \"components\":[]}\n",
    "for key in components:\n",
    "    data[\"id\"].append(components[key])\n",
    "    data[\"components\"].append(\", \".join(key.components))\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.to_csv(\"components_dataset.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
